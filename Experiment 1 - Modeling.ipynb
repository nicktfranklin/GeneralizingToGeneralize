{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# custom libraries used\n",
    "from models.grid_world import Experiment\n",
    "from models.agents import IndependentClusterAgent, JointClusteringAgent, FlatAgent\n",
    "from models.experiment_designs.experiment1 import gen_task_param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sims = 150\n",
    "\n",
    "# alpha is sample from the distribution\n",
    "# log(alpha) ~ N(alpha_mu, alpha_scale)\n",
    "alpha_mu = 0.0\n",
    "alpha_scale = 1.0\n",
    "\n",
    "inv_temp = 2.5\n",
    "goal_prior = 10e-20 \n",
    "prunning_threshold = 10.0\n",
    "evaluate = True\n",
    "\n",
    "np.random.seed(11223344)\n",
    "\n",
    "# pre generate a set of tasks for consistency. \n",
    "list_tasks = [gen_task_param() for _ in range(n_sims)]\n",
    "\n",
    "# pre draw the alphas for consistency\n",
    "list_alpha = [np.exp(scipy.random.normal(loc=alpha_mu, scale=alpha_scale)) \n",
    "              for _ in range(n_sims)]\n",
    "\n",
    "def sim_agent(AgentClass, name='None'):\n",
    "    results = []\n",
    "    for ii, (task_args, task_kwargs) in tqdm(enumerate(list_tasks), total=len(list_tasks)):\n",
    "        alpha = list_alpha[ii]\n",
    "        \n",
    "        agent = AgentClass(\n",
    "            Experiment(*task_args, **task_kwargs), \n",
    "            alpha=alpha, inv_temp=inv_temp, \n",
    "            goal_prior=goal_prior)\n",
    "        \n",
    "        _res = agent.generate(evaluate=evaluate, prunning_threshold=prunning_threshold)\n",
    "        _res[u'Model'] = name\n",
    "        _res[u'Iteration'] = [ii] * len(_res)\n",
    "        results.append(_res)\n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e685db19d7445e8742ab97dd800c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=150), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_ic = sim_agent(IndependentClusterAgent, name='Independent')\n",
    "results_jc = sim_agent(JointClusteringAgent, name='Joint')\n",
    "results_fl = sim_agent(JointClusteringAgent, name='Flat')\n",
    "results = pd.concat([results_ic, results_jc, results_fl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Predictions\n",
    "Summary Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Training'] = results.Context < 5\n",
    "results['Acc'] = pd.to_numeric(results.Reward)\n",
    "results[results['In Goal']].groupby(['Training', 'Model']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the overall rewards collected in the training and test contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_goal = results[results['In Goal'] ].copy()\n",
    "in_goal['Contexts'] = [None] * len(in_goal)\n",
    "in_goal.loc[in_goal.Context < 5, 'Contexts'] = 'Training'\n",
    "in_goal.loc[in_goal.Context >= 5, 'Contexts'] = 'Test'\n",
    "\n",
    "with sns.axes_style('white'):\n",
    "    g = sns.factorplot(y='Reward', data=in_goal, x='Contexts', \n",
    "                   hue='Model', units='Iteration', kind='bar', \n",
    "                   estimator=np.mean, palette='Accent', size=4)\n",
    "    sns.despine(offset=5, trim=False)\n",
    "    ax = g.axes[0][0]\n",
    "    ax.set_ylabel('Average reward per trial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break down the accuracy in each test context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('ticks'):\n",
    "    sns.factorplot(x='Context', y='Reward', data=in_goal[in_goal['Contexts'] == 'Test'], \n",
    "                   kind='bar', palette='Accent', col='Model', units='Iteration')\n",
    "    sns.despine(offset=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Figure\n",
    "*Left*: Reward collected in test contexts across time\n",
    "\n",
    "*Right*: Difference in reward collect between contexts across equated trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "# plot the difference scores\n",
    "df0 = results[results['In Goal'] & (results.Context > 4) & (results['Times Seen Context'] <= 4) &\n",
    "              (results['Model'] != 'Flat')\n",
    "             ].copy()\n",
    "\n",
    "score = list()\n",
    "for m in set(df0.Model):\n",
    "    for it in set(df0.Iteration):\n",
    "        subj_df = df0.loc[(df0.Model == m) & (df0.Iteration == it), :]\n",
    "                \n",
    "        diff_1 = subj_df.loc[subj_df.Context == 5, :].groupby('Times Seen Context').mean()['Acc'] - \\\n",
    "            subj_df.loc[(subj_df.Context == 6) | (subj_df.Context == 8), :].groupby('Times Seen Context').mean()['Acc']\n",
    "\n",
    "        diff_2 = subj_df.loc[(subj_df.Context == 7), :].groupby('Times Seen Context').mean()['Acc'] - \\\n",
    "            subj_df.loc[(subj_df.Context == 6) | (subj_df.Context == 8), :].groupby('Times Seen Context').mean()['Acc']\n",
    "\n",
    "        diff_3 = subj_df.loc[subj_df.Context == 6, :].groupby('Times Seen Context').mean()['Acc'] - \\\n",
    "            subj_df.loc[(subj_df.Context == 8), :].groupby('Times Seen Context').mean()['Acc']\n",
    "\n",
    "        n = len(diff_1)\n",
    "        score.append(pd.DataFrame({\n",
    "                    'Trials in Context': range(n),\n",
    "                    'Comparison': ['T1 vs T2/4'] * n,\n",
    "                    'Difference Score': diff_1,\n",
    "                    'Iteration': [it] * n,\n",
    "                    'Model': [m] * n,\n",
    "            }))\n",
    "        score.append(pd.DataFrame({\n",
    "                    'Trials in Context': range(n),\n",
    "                    'Comparison': ['T3 vs T2/4'] * n,\n",
    "                    'Difference Score': diff_2,\n",
    "                    'Iteration': [it] * n,\n",
    "                    'Model': [m] * n,\n",
    "            }))\n",
    "        score.append(pd.DataFrame({\n",
    "                    'Trials in Context': range(n),\n",
    "                    'Comparison': ['T2 vs T4'] * n,\n",
    "                    'Difference Score': diff_3,\n",
    "                    'Iteration': [it] * n,\n",
    "                    'Model': [m] * n,\n",
    "            }))\n",
    "score = pd.concat(score) \n",
    "\n",
    "df0 = results[results['In Goal'] & (results.Context >= 5) ].copy()\n",
    "df0['Context'] += 1\n",
    "\n",
    "with sns.axes_style('ticks'):\n",
    "    fig  = plt.figure(figsize=(5.5, 6.6))\n",
    "    gs = gridspec.GridSpec(2, 2, width_ratios=[2.5, 1], wspace=0.5, hspace=0.6)\n",
    "    axes = [\n",
    "        [plt.subplot(gs[0]), plt.subplot(gs[1])],\n",
    "        [plt.subplot(gs[2]),  plt.subplot(gs[3])]\n",
    "    ]\n",
    "    \n",
    "    sns.pointplot(x = 'Times Seen Context', hue='Context', y='Reward', # units='Iteration',\n",
    "                   data=df0[df0.Model=='Independent'],  palette='Set2', ax=axes[0][0])\n",
    "    sns.pointplot(x = 'Times Seen Context', hue='Context', y='Reward', # units='Iteration',\n",
    "                   data=df0[df0.Model=='Joint'],  palette='Set2', ax=axes[1][0])\n",
    "    \n",
    "    axes[0][0].legend_.remove() \n",
    "    ax = axes[1][0]\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    handles[0].set_label('T1')\n",
    "    handles[1].set_label('T2')\n",
    "    handles[2].set_label('T3')\n",
    "    handles[3].set_label('T4')\n",
    "    legend = ax.legend(title=\"Context\", loc='lower right', frameon=1)\n",
    "    legend.get_frame().set_edgecolor('w')\n",
    "    ax.get_legend().get_title().set_fontsize(12)\n",
    "    \n",
    "    \n",
    "    # plot difference scores on the right\n",
    "    sns.barplot(x='Comparison', y='Difference Score', \n",
    "            data=score[score.Model == 'Independent'], color='skyblue', ax=axes[0][1])\n",
    "    sns.barplot(x='Comparison', y='Difference Score', #units='Iteration',\n",
    "            data=score[score.Model == 'Joint'], color='skyblue', ax=axes[1][1])\n",
    "    \n",
    "    for ii in [0, 1]:\n",
    "        ax = axes[ii][0]\n",
    "        ax.set_ylabel(r'Accuracy',  fontdict={'size':12})\n",
    "        ax.plot([0, 5], [0.25, 0.25], 'k:')\n",
    "        ax.set_ylim([0, 1])\n",
    "    \n",
    "    sns.despine(offset=5)\n",
    "\n",
    "    for ii in [0, 1]:\n",
    "        ax = axes[ii][1]\n",
    "        ax.set_ylabel(r'Difference Score', labelpad=-3, fontdict={'size':12})\n",
    "        ax.set_xlabel('')\n",
    "        ax.plot([-1, 3], [0, 0], 'k:')\n",
    "        plt.sca(ax)\n",
    "        plt.xticks([0, 1, 2], [r'1 > 2', u'1/2 > 3/4', r'3 > 4' ], rotation='vertical',\n",
    "                  fontsize='small')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.agents import MetaAgent\n",
    "results_meta = sim_agent(MetaAgent, name='Meta')\n",
    "results_wMeta = pd.concat([results, results_meta])\n",
    "results_wMeta['Acc'] = pd.to_numeric(results_wMeta.Reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "# plot the difference scores\n",
    "df0 = results_wMeta[results_wMeta['In Goal'] & (results_wMeta.Context > 4) & \n",
    "                    (results_wMeta['Times Seen Context'] <= 4) &\n",
    "                    (results_wMeta['Model'] != 'Flat')\n",
    "                   ].copy()\n",
    "\n",
    "score = list()\n",
    "for m in set(df0.Model):\n",
    "    for it in set(df0.Iteration):\n",
    "        subj_df = df0.loc[(df0.Model == m) & (df0.Iteration == it), :]\n",
    "                \n",
    "        diff_1 = subj_df.loc[subj_df.Context == 5, :].groupby('Times Seen Context').mean()['Acc'] - \\\n",
    "            subj_df.loc[(subj_df.Context == 6) | (subj_df.Context == 8), :].groupby('Times Seen Context').mean()['Acc']\n",
    "\n",
    "        diff_2 = subj_df.loc[(subj_df.Context == 7), :].groupby('Times Seen Context').mean()['Acc'] - \\\n",
    "            subj_df.loc[(subj_df.Context == 6) | (subj_df.Context == 8), :].groupby('Times Seen Context').mean()['Acc']\n",
    "\n",
    "        diff_3 = subj_df.loc[subj_df.Context == 6, :].groupby('Times Seen Context').mean()['Acc'] - \\\n",
    "            subj_df.loc[(subj_df.Context == 8), :].groupby('Times Seen Context').mean()['Acc']\n",
    "\n",
    "        score.append(pd.DataFrame({\n",
    "                    'Trials in Context': range(n),\n",
    "                    'Comparison': ['T1 vs T2/4'] * n,\n",
    "                    'Difference Score': diff_1,\n",
    "                    'Iteration': [it] * n,\n",
    "                    'Model': [m] * n,\n",
    "            }))\n",
    "        score.append(pd.DataFrame({\n",
    "                    'Trials in Context': range(n),\n",
    "                    'Comparison': ['T3 vs T2/4'] * n,\n",
    "                    'Difference Score': diff_2,\n",
    "                    'Iteration': [it] * n,\n",
    "                    'Model': [m] * n,\n",
    "            }))\n",
    "        score.append(pd.DataFrame({\n",
    "                    'Trials in Context': range(n),\n",
    "                    'Comparison': ['T2 vs T4'] * n,\n",
    "                    'Difference Score': diff_3,\n",
    "                    'Iteration': [it] * n,\n",
    "                    'Model': [m] * n,\n",
    "            }))\n",
    "            \n",
    "\n",
    "\n",
    "score = pd.concat(score) \n",
    "\n",
    "df0 = results_wMeta[results_wMeta['In Goal'] & (results_wMeta.Context >= 5) ].copy()\n",
    "df0['Context'] += 1\n",
    "\n",
    "with sns.axes_style('ticks'):\n",
    "    fig  = plt.figure(figsize=(5.5, 3))\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[2.5, 1], wspace=0.5, hspace=0.6)\n",
    "    axes = [ plt.subplot(gs[0]), plt.subplot(gs[1])]\n",
    "    \n",
    "    sns.pointplot(x = 'Times Seen Context', hue='Context', y='Reward', # units='Iteration',\n",
    "                   data=df0[df0.Model=='Meta'],  palette='Set2', ax=axes[0])\n",
    "    \n",
    "    ax = axes[0]\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    handles[0].set_label('T1')\n",
    "    handles[1].set_label('T2')\n",
    "    handles[2].set_label('T3')\n",
    "    handles[3].set_label('T4')\n",
    "    legend = ax.legend(title=\"Context\", loc='lower right', frameon=1)\n",
    "    legend.get_frame().set_edgecolor('w')\n",
    "    ax.get_legend().get_title().set_fontsize(12)\n",
    "    \n",
    "    # plot difference scores on the right\n",
    "    sns.barplot(x='Comparison', y='Difference Score', \n",
    "            data=score[score.Model == 'Meta'], color='skyblue', ax=axes[1])\n",
    "    \n",
    "    ax = axes[0]\n",
    "    ax.set_ylabel(r'Accuracy',  fontdict={'size':12})\n",
    "    ax.plot([0, 5], [0.25, 0.25], 'k:')\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    sns.despine(offset=5)\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.set_ylabel(r'Difference Score', labelpad=-3, fontdict={'size':12})\n",
    "    ax.set_xlabel('')\n",
    "    ax.plot([-1, 3], [0, 0], 'k:')\n",
    "    plt.sca(ax)\n",
    "    plt.xticks([0, 1, 2], [r'1 > 2', u'1/2 > 3/4', r'3 > 4' ], rotation='vertical',\n",
    "              fontsize='small')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the evolution of the model responsibilities over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context(u'seaborn-whitegrid'):\n",
    "\n",
    "    df0 = results_wMeta[(results_wMeta['Steps Taken']==1) & \n",
    "                  (results_wMeta['Model'] == 'Meta')\n",
    "                 ].copy()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    ax = axes[0]\n",
    "    x = range(1, int(df0['Trial Number'].max()) + 2)\n",
    "    y = df0.groupby('Trial Number')['Joint Probability'].mean()\n",
    "    y_err = df0.groupby('Trial Number')['Joint Probability'].std() / np.sqrt(n_sims)\n",
    "    ax.plot(x, y, 'k-')\n",
    "    ax.fill_between(x, y-y_err, y+y_err, alpha=0.25, color='k')\n",
    "    ax.set_ylabel('Probability of Joint Agent as Actor')\n",
    "    ax.set_xlabel('Trial')\n",
    "\n",
    "    ax = axes[1]\n",
    "    y = df0.groupby('Trial Number')['Ind Weight'].mean()\n",
    "    y_err = df0.groupby('Trial Number')['Ind Weight'].std() / np.sqrt(n_sims)\n",
    "    ax.plot(x, y, 'r-')\n",
    "    ax.fill_between(x, y-y_err, y+y_err, alpha=0.2, color='r')\n",
    "\n",
    "    y = df0.groupby('Trial Number')['Joint Weight'].mean()\n",
    "    y_err = df0.groupby('Trial Number')['Joint Weight'].std() / np.sqrt(n_sims)\n",
    "    ax.plot(x, y, '-')\n",
    "    ax.fill_between(x, y-y_err, y+y_err, alpha=0.2)\n",
    "    plt.legend(loc='best')\n",
    "    ax.set_xlabel('Trial')\n",
    "    ax.set_ylabel('Unnormalized log Posterior')\n",
    "    plt.subplots_adjust(hspace=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_wMeta.to_pickle('Experiment1sim.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
